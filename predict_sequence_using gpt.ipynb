{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "#use this from Char_To_INT.txt File \n",
    "CHAR_TO_INT = {'G': 0, 'I': 1, 'E': 2, 'R': 3, 'L': 4, 'Q': 5, 'S': 6, 'X': 7, 'B': 8, 'D': 9, 'F': 10, 'N': 11, 'M': 12, 'T': 13, 'Y': 14, 'P': 15, 'C': 16, 'V': 17, 'Z': 18, '-': 19, 'H': 20, 'A': 21, 'W': 22, 'K': 23}\n",
    "# Define the same configuration and tokenizer used during training\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name, n_positions=512)\n",
    "class GPT2ForProteinPrediction(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, NUM_CLASSES, bias=False)\n",
    "# Recreate the model instance with the same architecture\n",
    "NUM_CLASSES = len(CHAR_TO_INT)  \n",
    "model = GPT2ForProteinPrediction(config).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('protein_gpt_model_mab.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---MTQSPSSISASVGDRVTITCK---NIDKYINWYQQKPGKAPKIIIYNTNNIQTGVPSRF---G----FTFTI-----------YCIQHISRPRTFGQGTKVEIKRSIAAPSVFIFPPSDEQIKSGTASVVCIINNFYPREAQPRRKVDNAIQSGNSQESVTEQDSKDSTYSISSTITISKADYEKHKVYACEVTHQGISSPVTKSFN----\n",
      "Predicted Sequence: DIQMTQSPSSLSASVGDRVTITCKASQNIDKYLNWYQQKPGKAPKLLIYNTNNLQTGVPSRFSGSGSGTDFTFTISSLQPEDIATYYCLQHISRPRTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def introduce_gaps_and_errors(sequence, gap_probability=0.1, error_probability=0.05):\n",
    "    new_sequence = []\n",
    "    valid_amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    for aa in sequence:\n",
    "        if np.random.rand() < gap_probability:\n",
    "            new_sequence.append('-')\n",
    "        else:\n",
    "            if np.random.rand() < error_probability and aa in valid_amino_acids:\n",
    "                new_sequence.append(random.choice(list(valid_amino_acids - {aa})))\n",
    "            else:\n",
    "                new_sequence.append(aa)\n",
    "    return ''.join(new_sequence)\n",
    "\n",
    "def custom_tokenize_pred(sequence, char_to_int):\n",
    "    token_ids = [char_to_int.get(char, char_to_int.get('-')) for char in sequence]\n",
    "    max_length = len(token_ids)\n",
    "    padded_token_ids = token_ids + [char_to_int.get('-')] * (max_length - len(token_ids))\n",
    "    return padded_token_ids\n",
    "def predict_sequence(input_sequence):\n",
    "    input_tokens = torch.tensor([custom_tokenize_pred(input_sequence, CHAR_TO_INT)], dtype=torch.long).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tokens)\n",
    "        logits = outputs[0]\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    predicted_sequence = ''.join([INT_TO_CHAR.get(id, '-') for id in predicted_token_ids])\n",
    "    return predicted_sequence\n",
    "\n",
    "\n",
    "#use this from INT_To_CHAR.txt  File \n",
    "INT_TO_CHAR = {0: 'G', 1: 'I', 2: 'E', 3: 'R', 4: 'L', 5: 'Q', 6: 'S', 7: 'X', 8: 'B', 9: 'D', 10: 'F', 11: 'N', 12: 'M', 13: 'T', 14: 'Y', 15: 'P', 16: 'C', 17: 'V', 18: 'Z', 19: '-', 20: 'H', 21: 'A', 22: 'W', 23: 'K'}\n",
    "input_seq =\"DIQMTQSPSSLSASVGDRVTITCKASQNIDKYLNWYQQKPGKAPKLLIYNTNNLQTGVPS\\\n",
    "RFSGSGSGTDFTFTISSLQPEDIATYYCLQHISRPRTFGQGTKVEIKRTVAAPSVFIFPP\\\n",
    "SDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLT\\\n",
    "LSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC\"\n",
    "# scaffold_seq = introduce_gaps_and_errors(input_seq, gap_probability=(0.3) , error_probability=0.10)\n",
    "scaffold_seq = \"---MTQSPSSISASVGDRVTITCK---NIDKYINWYQQKPGKAPKIIIYNTNNIQTGVPSRF---G----FTFTI-----------YCIQHISRPRTFGQGTKVEIKRSIAAPSVFIFPPSDEQIKSGTASVVCIINNFYPREAQPRRKVDNAIQSGNSQESVTEQDSKDSTYSISSTITISKADYEKHKVYACEVTHQGISSPVTKSFN----\"\n",
    "print(scaffold_seq)\n",
    "predicted_seq = predict_sequence(scaffold_seq)\n",
    "print(\"Predicted Sequence:\", predicted_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mD\u001b[0m\u001b[92mI\u001b[0m\u001b[92mQ\u001b[0mMTQSPSS\u001b[92mL\u001b[0mSASVGDRVTITCK\u001b[92mA\u001b[0m\u001b[92mS\u001b[0m\u001b[92mQ\u001b[0mNIDKY\u001b[92mL\u001b[0mNWYQQKPGKAPK\u001b[92mL\u001b[0m\u001b[92mL\u001b[0mIYNTNN\u001b[92mL\u001b[0mQTGVPSRF\u001b[92mS\u001b[0m\u001b[92mG\u001b[0m\u001b[92mS\u001b[0mG\u001b[92mS\u001b[0m\u001b[92mG\u001b[0m\u001b[92mT\u001b[0m\u001b[92mD\u001b[0mFTFTI\u001b[92mS\u001b[0m\u001b[92mS\u001b[0m\u001b[92mL\u001b[0m\u001b[92mQ\u001b[0m\u001b[92mP\u001b[0m\u001b[92mE\u001b[0m\u001b[92mD\u001b[0m\u001b[92mI\u001b[0m\u001b[92mA\u001b[0m\u001b[92mT\u001b[0m\u001b[92mY\u001b[0mYC\u001b[92mL\u001b[0mQHISRPRTFGQGTKVEIKR\u001b[92mT\u001b[0m\u001b[92mV\u001b[0mA\n",
      "APSVFIFPPSDEQ\u001b[92mL\u001b[0mKSGTASVVC\u001b[92mL\u001b[0m\u001b[92mL\u001b[0mNNFYPREA\u001b[92mK\u001b[0m\u001b[92mV\u001b[0m\u001b[92mQ\u001b[0m\u001b[92mW\u001b[0mKVDNA\u001b[92mL\u001b[0mQSGNSQESVTEQDSKDSTYS\u001b[92mL\u001b[0mSST\u001b[92mL\u001b[0mT\u001b[92mL\u001b[0mSKADYEKHKVYACEVTHQG\u001b[92mL\u001b[0mSSPVTKSFN\u001b[92mR\u001b[0m\u001b[92mG\u001b[0m\u001b[92mE\u001b[0m\u001b[92mC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seq = \"\"\n",
    "for i in range(0,len(input_seq)):\n",
    "    if scaffold_seq[i] == \"-\" and input_seq[i]== predicted_seq[i]:\n",
    "        seq+= \"\\033[92m\" + predicted_seq[i]+ \"\\033[0m\"\n",
    "\n",
    "\n",
    "    elif input_seq[i]== predicted_seq[i]:   \n",
    "        if   scaffold_seq[i]== input_seq[i]: \n",
    "            seq+= predicted_seq[i]\n",
    "        else:\n",
    "            seq+= \"\\033[92m\" + predicted_seq[i]+ \"\\033[0m\"\n",
    "        # print(predicted_seq[i])\n",
    "    else:  \n",
    "        seq+= \"\\033[91m\" + predicted_seq[i] + \"\\033[0m\"\n",
    "        # print(\"\\033[91m\" + predicted_seq[i] + \"\\033[0m\")\n",
    "    if i==110:\n",
    "        seq+=\"\\n\"\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input seq:\n",
      "\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0mMTQSPSS\u001b[91mI\u001b[0mSASVGDRVTITCK\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0mNIDKY\u001b[91mI\u001b[0mNWYQQKPGKAPK\u001b[91mI\u001b[0m\u001b[91mI\u001b[0mIYNTNN\u001b[91mI\u001b[0mQTGVPSRF\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0mG\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0mFTFTI\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0mYC\u001b[91mI\u001b[0mQHISRPRTFGQGTKVEIKR\u001b[91mS\u001b[0m\u001b[91mI\u001b[0mA\n",
      "APSVFIFPPSDEQ\u001b[91mI\u001b[0mKSGTASVVC\u001b[91mI\u001b[0m\u001b[91mI\u001b[0mNNFYPREA\u001b[91mQ\u001b[0m\u001b[91mP\u001b[0m\u001b[91mR\u001b[0m\u001b[91mR\u001b[0mKVDNA\u001b[91mI\u001b[0mQSGNSQESVTEQDSKDSTYS\u001b[91mI\u001b[0mSST\u001b[91mI\u001b[0mT\u001b[91mI\u001b[0mSKADYEKHKVYACEVTHQG\u001b[91mI\u001b[0mSSPVTKSFN\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-\u001b[0m\n",
      "\n",
      "predicted seq:\n",
      "\u001b[92mD\u001b[0m\u001b[92mI\u001b[0m\u001b[92mQ\u001b[0mMTQSPSS\u001b[92mL\u001b[0mSASVGDRVTITCK\u001b[92mA\u001b[0m\u001b[92mS\u001b[0m\u001b[92mQ\u001b[0mNIDKY\u001b[92mL\u001b[0mNWYQQKPGKAPK\u001b[92mL\u001b[0m\u001b[92mL\u001b[0mIYNTNN\u001b[92mL\u001b[0mQTGVPSRF\u001b[92mS\u001b[0m\u001b[92mG\u001b[0m\u001b[92mS\u001b[0mG\u001b[92mS\u001b[0m\u001b[92mG\u001b[0m\u001b[92mT\u001b[0m\u001b[92mD\u001b[0mFTFTI\u001b[92mS\u001b[0m\u001b[92mS\u001b[0m\u001b[92mL\u001b[0m\u001b[92mQ\u001b[0m\u001b[92mP\u001b[0m\u001b[92mE\u001b[0m\u001b[92mD\u001b[0m\u001b[92mI\u001b[0m\u001b[92mA\u001b[0m\u001b[92mT\u001b[0m\u001b[92mY\u001b[0mYC\u001b[92mL\u001b[0mQHISRPRTFGQGTKVEIKR\u001b[92mT\u001b[0m\u001b[92mV\u001b[0mA\n",
      "APSVFIFPPSDEQ\u001b[92mL\u001b[0mKSGTASVVC\u001b[92mL\u001b[0m\u001b[92mL\u001b[0mNNFYPREA\u001b[92mK\u001b[0m\u001b[92mV\u001b[0m\u001b[92mQ\u001b[0m\u001b[92mW\u001b[0mKVDNA\u001b[92mL\u001b[0mQSGNSQESVTEQDSKDSTYS\u001b[92mL\u001b[0mSST\u001b[92mL\u001b[0mT\u001b[92mL\u001b[0mSKADYEKHKVYACEVTHQG\u001b[92mL\u001b[0mSSPVTKSFN\u001b[92mR\u001b[0m\u001b[92mG\u001b[0m\u001b[92mE\u001b[0m\u001b[92mC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seq1= \"\"\n",
    "for i in range(0,len(input_seq)):\n",
    "    if scaffold_seq[i]== input_seq[i]:   \n",
    "        seq1+= scaffold_seq[i]\n",
    "        # print(predicted_seq[i])\n",
    "    else:  \n",
    "        seq1+= \"\\033[91m\" + scaffold_seq[i] + \"\\033[0m\"\n",
    "        # print(\"\\033[91m\" + predicted_seq[i] + \"\\033[0m\")\n",
    "    if i==110:\n",
    "        seq1+=\"\\n\"\n",
    "print(\"input seq:\\n\"+seq1)\n",
    "print(\"\\npredicted seq:\\n\"+seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
